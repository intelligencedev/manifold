# ===================================================================
# Manifold Configuration with Environment Variable Support
# ===================================================================
#
# This configuration file contains default values for Manifold.
# All settings can be overridden at runtime using environment variables.
#
# ENVIRONMENT VARIABLE MAPPING:
# ----------------------------
# Environment variables are automatically mapped to YAML paths using this convention:
#
# 1. Prefix all variables with "MANIFOLD__"
# 2. Use UPPERCASE for all letters 
# 3. Use DOUBLE underscore (__) to separate YAML hierarchy levels
# 4. Use SINGLE underscore (_) for keys containing underscores
#
# Examples:
#   MANIFOLD__HOST=api.example.com                    → host: 'api.example.com'
#   MANIFOLD__PORT=9000                               → port: 9000
#   MANIFOLD__DATABASE__CONNECTION_STRING=...         → database.connection_string: '...'
#   MANIFOLD__MCPSERVERS__GITHUB__COMMAND=docker      → mcpServers.github.command: 'docker'
#   MANIFOLD__COMPLETIONS__DEFAULT_HOST=http://...    → completions.default_host: 'http://...'
#
# VALUE TYPES:
# -----------
# Different value types are automatically handled as below:
#
# - Numbers:     MANIFOLD__PORT=8080                   → port: 8080
# - Booleans:    MANIFOLD__SINGLE_NODE_INSTANCE=false  → single_node_instance: false
# - Strings:     MANIFOLD__HOST=localhost              → host: 'localhost'
# - Null:        MANIFOLD__HF_TOKEN=null               → hf_token: null
# - JSON arrays: MANIFOLD__MCPSERVERS__GITHUB__ARGS='["run","--rm"]' 
#                → mcpservers.github.args: ["run","--rm"]
# - JSON objects: MANIFOLD__SOME__CONFIG='{"key":"value"}' 
#                → some.config: {"key":"value"}
#
# HOW IT WORKS:
# ------------
# At container startup, the process_config.sh script:
# 1. Copies this file to config.yaml
# 2. Finds all MANIFOLD__* environment variables
# 3. Maps them to their corresponding YAML paths
# 4. Updates the config.yaml file accordingly
#
# ===================================================================

# ===================================================================
# SERVER CONFIGURATION
# ===================================================================

# Server address and port
host: 'localhost'
port: 8080

# Storage path for models, database files, and other persistent data
data_path: '/data'

# ===================================================================
# RUNTIME CONFIGURATION
# ===================================================================

# When enabled, Manifold automatically runs llama-server instances for:
# - embeddings (port 32184)
# - reranker (port 32185)
# - completions (port 32186)
single_node_instance: true

# ===================================================================
# DATABASE CONFIGURATION
# ===================================================================

database:
  # PostgreSQL connection string with PGVector extension
  # Format: postgres://username:password@hostname:port/database?sslmode=disable
  connection_string: ""

# ===================================================================
# API TOKENS
# ===================================================================

# HuggingFace API token for accessing gated models
hf_token: ""

# Google Gemini API token
google_gemini_key: ""

# Anthropic API token (Claude models)
anthropic_key: ""

# ===================================================================
# LLM SERVICES CONFIGURATION
# ===================================================================

# Completions Service Configuration
# Handles generation of text completions based on input prompts
completions:
  default_host: "http://127.0.0.1:32186/v1/chat/completions" # or https://api.openai.com/v1/chat/completions
  completions_model: 'gpt-4o' # ignored if using local endpoint
  api_key: "" # Used with OpenAI API if configured as default host
  # Agent configuration for ReAct framework
  agent:
    max_steps: 100  # Maximum number of steps the agent will take before terminating
    memory: false   # Legacy memory setting (will be deprecated)

# Example CLI command for running the embeddings service manually:
# llama-server -m <data_path>/models/embeddings/nomic-embed-text-v1.5.Q8_0.gguf -c 65536 -np 8 -b 8192 -ub 8192 -fa --host 127.0.0.1 --port 32184 -lv 1 --embedding
#
# Embeddings API Configuration
# Using local nomic-embed-text-v1.5
# The initialize process will automatically download and start the model at port 32184
embeddings:
  host: "http://127.0.0.1:32184/v1/embeddings"
  # OpenAI API compatible API key, not required for local servers unless configured on that server
  api_key: ""

# Embeddings Service Configuration
# Manages the creation of vector representations for text data
embeddings:
  # OpenAI-compatible API endpoint
  host: "http://127.0.0.1:32184/v1/embeddings"
  # API key for the embeddings service (if required)
  api_key: ""
  # Vector dimensions for the embedding model
  dimensions: 768
  # Prefix added to document text before embedding
  embed_prefix: "search_document: "
  # Prefix added to query text before embedding
  search_prefix: "search_query: "

# Reranker Service Configuration
# Reorders search results to improve relevance
reranker:
  # OpenAI-compatible API endpoint
  host: "http://127.0.0.1:32185/v1/rerank"

# Agentic Memory Configuration
# Controls the long-term memory capabilities of the Manifold agent
agentic_memory:
  enabled: false  # Set to true to enable long-term memory across agent sessions
                  # When enabled, agents can recall past reasoning, actions, and observations
                  # This improves consistency and continuity in multi-turn interactions
                  # Requires PostgreSQL with pgvector extension for vector storage

# List of external MCP servers
# Example GitHub MCP server
mcpServers:
  # Manifold's built-in MCP server
  manifold:
    command: ./cmd/mcp-manifold/mcp-manifold
    args: []
    env: {}

  # Example GitHub MCP server (commented by default)
  # github:
  #   command: docker
  #   args:
  #     - run
  #     - -i
  #     - --rm
  #     - -e
  #     - GITHUB_PERSONAL_ACCESS_TOKEN
  #     - ghcr.io/github/github-mcp-server
  #   env:
  #     GITHUB_PERSONAL_ACCESS_TOKEN: ""