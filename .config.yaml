# Manifold Example Configuration

# Manifold Host
host: 'localhost'
port: 8080

# Manifold storage path: models, database files, etc
data_path: '~/.manifold'

# Database Configuration (PGVector)
database:
  connection_string: "postgres://pgadmin:yourpassword@localhost:5432/manifold?sslmode=disable"  # REPLACE with your actual credentials

# HuggingFace Token
hf_token: "..." 

# Anthropic API token
anthropic_key: "..."

# Default Completions Configuration - any openai api compatible backend - llama.cpp (llama-server), vllm, mlx_lm.server, etc
completions:
  default_host: "<my_openai_api_compatible_server>/v1/chat/completions"
  # OpenAI API compatible API key, not required for local servers unless configured on that server
  api_key: "my_api_key"

# Embeddings API Configuration
# Manifold is tested with https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF
# llama-server \
# --model ./models/nomic-embed-v1.5/nomic-embed-text-v1.5.Q8_0.gguf \
# --port 32184 --host 0.0.0.0 \
# --gpu-layers 99 --ctx-size 16384 \
# --ubatch-size 16384 --batch-size 4096 \
# --embeddings --pooling mean --mlock \
# --threads 16 --parallel 10 --threads-http 10 --flash-attn

embeddings:
  host: "<my_openai_api_compatible_server>/v1/embeddings"
  # OpenAI API compatible API key, not required for local servers unless configured on that server
  api_key: "your_embeddings_api_key"
  dimensions: 768 # Size of embedding dimensions
  embed_prefix: "search_document: "
  search_prefix: "search_query: "

# Reranker llama.cpp endpoint
# Reranker llama.cpp endpoint
# llama-server \
#     -m ./models/rerankers/slide-bge-reranker-v2-m3.Q8_0.gguf \
#     -c 65536 -np 8 -b 8192 -ub 8192 -fa \
#     --host 0.0.0.0 --port 32185 -lv 1 \
#     --reranking --pooling rank
reranker:
  host: "<my llama.cpp or other /v1/rerank endpoint>"
