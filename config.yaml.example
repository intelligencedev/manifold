# Authentication (OIDC or OAuth2) — optional
# Default false
auth:
  enabled: false
  provider: oauth2
  #issuerURL: "http://localhost:8083/realms/manifold"
  clientID: "github client id"
  clientSecret: "github client secret"
  redirectURL: "http://localhost:32180/auth/callback"
  allowedDomains: []
  cookieName: "manifold_session"
  cookieSecure: false # set true in production (HTTPS)
  cookieDomain: ""
  stateTTLSeconds: 600
  sessionTTLHours: 72
  # Optional OAuth2-only settings when provider: oauth2
  oauth2:
    authURL: "https://github.com/login/oauth/authorize"
    tokenURL: "https://github.com/login/oauth/access_token"
    userInfoURL: "https://api.github.com/user"
    logoutURL: ""
    logoutRedirectParam: "redirect_uri"
    scopes: [read:user, user:email]
    providerName: "github"
    defaultRoles: [user]
    emailField: "email"
    nameField: "name"
    pictureField: "avatar_url"
    subjectField: "id"
    rolesField: ""

llm_client:
  # Options: openai (default), anthropic, google, local
  provider: openai
  openai:
    api: completions
    summaryBaseURL: "https://api.openai.com/v1"
    summaryModel: gpt-5-nano
    # Optional: request-wide headers for the main agent
    # extraHeaders:
    #   X-App-Tenant: example
    extraParams:
      parallel_tool_calls: true
      prompt_cache_key: manifold-
      reasoning_effort: medium
  # anthropic:
  #   apiKey: ""
  #   model: ""
  #   baseURL: ""
  # google:
  #   apiKey: ""
  #   model: "gemini-1.5-flash"
  #   baseURL: "https://generativelanguage.googleapis.com/"

# No specialists configured for first launch
specialists: []

# Agent runtime configuration
maxSteps: 1000                 # Maximum reasoning steps per agent run
maxToolParallelism: 0          # Tool concurrency (0=unbounded, 1=sequential, >1=capped)

# Chat summarization configuration (token-based only)
# All summarization uses a reserve buffer pattern to protect output tokens
summaryEnabled: true
# Reserve buffer for output tokens (including reasoning tokens for reasoning models)
# Standard chat: 8,000-16,000 | Reasoning models (o1/o3): ~25,000 | Code generation: 16,000-32,000
summaryReserveBufferTokens: 25000
# Minimum number of recent messages to keep in raw form when summarizing
summaryMinKeepLastMessages: 4
# Maximum tokens per summary chunk when building multi-turn summaries
summaryMaxSummaryChunkTokens: 4096

obs:
  serviceName: manifold
  environment: local
  otlp: "http://otel-collector:4318"
  clickhouse:
    dsn: "tcp://clickhouse:9000?database=otel"
    metricsTable: metrics_sum
    tracesTable: traces
    logsTable: logs
    timestampColumn: TimeUnix
    valueColumn: Value
    modelAttributeKey: llm.model
    promptMetricName: llm.prompt_tokens
    completionMetricName: llm.completion_tokens
    lookbackHours: 24
    timeoutSeconds: 5

# Evolving Memory System (Search → Synthesis → Evolve)
# Based on https://arxiv.org/abs/2511.20857
# Requires embedding service configured below
evolvingMemory:
  enabled: false             # Enable evolving memory system
  maxSize: 1000              # Max memory entries (automatic pruning)
  topK: 4                    # Number of similar experiences to retrieve
  windowSize: 20             # Sliding window size for ExpRecent mode
  enableRAG: true            # Enable ExpRAG similarity-based retrieval
  reMemEnabled: false        # Enable Think-Act-Refine mode (advanced)
  maxInnerSteps: 5           # Max THINK/REFINE loops in ReMem mode
  model: "gpt-4o-mini"       # Model for memory summarization
  # Smart pruning (advanced) - deduplication and relevance-based memory management
  enableSmartPrune: false    # Enable similarity-based dedup & relevance pruning
  pruneThreshold: 0.95       # Similarity threshold for duplicate detection (0.0-1.0)
  relevanceDecay: 0.99       # Daily decay factor for relevance scores (0.0-1.0)
  minRelevance: 0.1          # Minimum relevance to avoid pruning (0.0-1.0)

# Embedding service (required for evolving memory)
embedding:
  baseURL: "http://localhost:11434"
  model: "nomic-embed-text"
  path: "/api/embeddings"
  timeoutSeconds: 30
  # For OpenAI-compatible services:
  # apiKey: "${OPENAI_API_KEY}"
  # apiHeader: "Authorization"

databases:
  defaultDSN: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
  chat:
    backend: postgres  # memory | auto | postgres
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
  search:
    backend: postgres   # memory | postgres
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
  vector:
    backend: postgres   # memory | postgres | qdrant
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
    #dimensions: 1536  # optional; for validation in some backends
    metric: cosine    # optional; cosine | dot | l2 (backend‑specific)
  graph:
    backend: postgres   # memory | postgres
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"

mcp:
  servers:
  - name: sequentialthinking
    command: docker
    args:
    - run
    - -i
    - --rm
    - mcp/sequentialthinking

  - name: duckduckgo
    command: docker
    args:
      - run
      - -i
      - --rm
      - mcp/duckduckgo

  # Remote HTTP streamable example
  - name: acme
    url: https://mcp.acme.com/mcp
    origin: https://manifold.local
    bearerToken: ${ACME_MCP_TOKEN}
    headers:
      X-Client: Manifold
    http:
      timeoutSeconds: 30
      # proxyURL: http://proxy.local:3128
      tls:
        insecureSkipVerify: false
        # caFile: /etc/ssl/certs/ca-bundle.crt
# Projects Storage Configuration (Phase 0 - Feature Flags)
# These settings prepare for S3-backed durable storage and ephemeral workspaces.
# In Phase 0, only "filesystem" backend and "legacy" workspace mode are active.
projects:
  # Backend: "filesystem" (default) | "s3" (future)
  backend: filesystem
  # Enable at-rest encryption for project files
  encrypt: false
  # Workspace configuration for agent tool execution
  workspace:
    # Mode: "legacy" (direct project dir) | "ephemeral" (per-session copies, future)
    mode: legacy
    # Root directory for ephemeral workspaces (default: ${WORKDIR}/sandboxes)
    # root: "${WORKDIR}/sandboxes"
    # TTL for ephemeral workspaces in seconds (default: 86400 = 24 hours)
    ttlSeconds: 86400
  # S3/MinIO configuration (used when backend: s3)
  # s3:
  #   endpoint: "http://minio:9000"
  #   region: "us-east-1"
  #   bucket: "manifold-workspaces"
  #   prefix: "workspaces"
  #   accessKey: "${PROJECTS_S3_ACCESS_KEY}"
  #   secretKey: "${PROJECTS_S3_SECRET_KEY}"
  #   usePathStyle: true          # Required for MinIO
  #   tlsInsecureSkipVerify: false
  #   sse:
  #     mode: none                 # none | sse-s3 | sse-kms
  #     kmsKeyID: ""