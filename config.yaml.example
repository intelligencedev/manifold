# Authentication (OIDC or OAuth2) — optional
# Default false
auth:
  enabled: false
  provider: oauth2
  #issuerURL: "http://localhost:8083/realms/manifold"
  clientID: "github client id"
  clientSecret: "github client secret"
  redirectURL: "http://localhost:32180/auth/callback"
  allowedDomains: []
  cookieName: "manifold_session"
  cookieSecure: false # set true in production (HTTPS)
  cookieDomain: ""
  stateTTLSeconds: 600
  sessionTTLHours: 72
  # Optional OAuth2-only settings when provider: oauth2
  oauth2:
    authURL: "https://github.com/login/oauth/authorize"
    tokenURL: "https://github.com/login/oauth/access_token"
    userInfoURL: "https://api.github.com/user"
    logoutURL: ""
    logoutRedirectParam: "redirect_uri"
    scopes: [read:user, user:email]
    providerName: "github"
    defaultRoles: [user]
    emailField: "email"
    nameField: "name"
    pictureField: "avatar_url"
    subjectField: "id"
    rolesField: ""

llm_client:
  # Options: openai (default), anthropic, google, local
  provider: openai
  openai:
    api: completions
    summaryBaseURL: "https://api.openai.com/v1"
    summaryModel: gpt-5-nano
    # Optional: request-wide headers for the main agent
    # extraHeaders:
    #   X-App-Tenant: example
    extraParams:
      parallel_tool_calls: true
      prompt_cache_key: manifold-
      reasoning_effort: medium
  # anthropic:
  #   apiKey: ""
  #   model: ""
  #   baseURL: ""
  #   promptCache:
  #     enabled: false
  #     # When enabled, Manifold attaches Anthropic cache_control directives.
  #     # If enabled with no scope set, defaults to cacheSystem+cacheTools.
  #     cacheSystem: true
  #     cacheTools: true
  #     cacheMessages: false
  # google:
  #   apiKey: ""
  #   model: "gemini-1.5-flash"
  #   baseURL: "https://generativelanguage.googleapis.com/"

# No specialists configured for first launch
specialists: []

# Agent runtime configuration
maxSteps: 1000                 # Maximum reasoning steps per agent run
maxToolParallelism: 0          # Tool concurrency (0=unbounded, 1=sequential, >1=capped)

# Chat summarization configuration (token-based only)
# All summarization uses a reserve buffer pattern to protect output tokens
summaryEnabled: true
# Optional: cap the context window used for chat-memory budgeting.
# This keeps the raw transcript short even when the underlying model supports
# very large context windows.
# Env override: SUMMARY_CONTEXT_WINDOW_TOKENS
summaryContextWindowTokens: 32000
# Reserve buffer for output tokens (including reasoning tokens for reasoning models)
# Standard chat: 8,000-16,000 | Reasoning models (o1/o3): ~25,000 | Code generation: 16,000-32,000
summaryReserveBufferTokens: 25000
# Minimum number of recent messages to keep in raw form when summarizing
summaryMinKeepLastMessages: 4
# Maximum number of recent messages to keep in raw form.
# If the chat grows beyond this, older turns are rolled into the session summary
# even if they still fit in the context window.
# Env override: SUMMARY_MAX_KEEP_LAST_MESSAGES
summaryMaxKeepLastMessages: 12
# Maximum tokens per summary chunk when building multi-turn summaries
summaryMaxSummaryChunkTokens: 4096

obs:
  serviceName: manifold
  environment: local
  otlp: "http://otel-collector:4318"
  clickhouse:
    dsn: "tcp://clickhouse:9000?database=otel"
    metricsTable: metrics_sum
    tracesTable: traces
    logsTable: logs
    timestampColumn: TimeUnix
    valueColumn: Value
    modelAttributeKey: llm.model
    promptMetricName: llm.prompt_tokens
    completionMetricName: llm.completion_tokens
    lookbackHours: 24
    timeoutSeconds: 5

# Evolving Memory System (Search → Synthesis → Evolve)
# Based on https://arxiv.org/abs/2511.20857
# Requires embedding service configured below
evolvingMemory:
  enabled: false             # Enable evolving memory system
  maxSize: 1000              # Max memory entries (automatic pruning)
  topK: 4                    # Number of similar experiences to retrieve
  windowSize: 20             # Sliding window size for ExpRecent mode
  enableRAG: true            # Enable ExpRAG similarity-based retrieval
  reMemEnabled: false        # Enable Think-Act-Refine mode (advanced)
  maxInnerSteps: 5           # Max THINK/REFINE loops in ReMem mode
  model: "gpt-4o-mini"       # Model for memory summarization
  # Smart pruning (advanced) - deduplication and relevance-based memory management
  enableSmartPrune: false    # Enable similarity-based dedup & relevance pruning
  pruneThreshold: 0.95       # Similarity threshold for duplicate detection (0.0-1.0)
  relevanceDecay: 0.99       # Daily decay factor for relevance scores (0.0-1.0)
  minRelevance: 0.1          # Minimum relevance to avoid pruning (0.0-1.0)

# Embedding service (required for evolving memory)
embedding:
  baseURL: "http://localhost:11434"
  model: "nomic-embed-text"
  path: "/api/embeddings"
  timeoutSeconds: 30
  # For OpenAI-compatible services:
  # apiKey: "${OPENAI_API_KEY}"
  # apiHeader: "Authorization"

databases:
  defaultDSN: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
  chat:
    backend: postgres  # memory | auto | postgres
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
  search:
    backend: postgres   # memory | postgres
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
  vector:
    backend: postgres   # memory | postgres | qdrant
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"
    #dimensions: 1536  # optional; for validation in some backends
    metric: cosine    # optional; cosine | dot | l2 (backend‑specific)
  graph:
    backend: postgres   # memory | postgres
    dsn: "postgres://manifold:manifold@pg-manifold:5432/manifold?sslmode=disable"

# MCP (Model Context Protocol) Server Configuration
# 
# Servers can be:
# - Stdio-based: Local processes spawned by Manifold (command + args)
# - Remote HTTP: Streamable HTTP endpoints (url)
#
# Path-Dependent Servers:
# When auth is enabled, servers marked with `pathDependent: true` will be
# instantiated per-user with {{PROJECT_DIR}} expanded to the user's active
# project workspace path.
# NOTE: Use {{PROJECT_DIR}} NOT ${PROJECT_DIR} - the config loader runs os.ExpandEnv
# which would expand ${...} syntax prematurely.
mcp:
  servers:
  # Stateless server - shared globally (default behavior)
  - name: sequentialthinking
    command: docker
    args:
    - run
    - -i
    - --rm
    - mcp/sequentialthinking

  # Stateless server - shared globally
  - name: duckduckgo
    command: docker
    args:
      - run
      - -i
      - --rm
      - mcp/duckduckgo

  # Path-dependent server - per-user instances in enterprise mode
  # The {{PROJECT_DIR}} placeholder is expanded to user's workspace path
  # - name: filesystem
  #   pathDependent: true
  #   command: docker
  #   args:
  #     - run
  #     - -i
  #     - --rm
  #     - -v
  #     - "{{PROJECT_DIR}}:/app/files"
  #     - mcp/filesystem
  #     - /app/files

  # Path-dependent server with env var placeholder
  # - name: ast_grep
  #   pathDependent: true
  #   command: docker
  #   args:
  #     - run
  #     - -i
  #     - --rm
  #     - -v
  #     - "${PROJECT_DIR}:/src"
  #     - mcp/ast-grep
  #   env:
  #     WORKSPACE_ROOT: "${PROJECT_DIR}"

  # Remote HTTP streamable example - always shared (no filesystem access)
  - name: acme
    url: https://mcp.acme.com/mcp
    origin: https://manifold.local
    bearerToken: ${ACME_MCP_TOKEN}
    headers:
      X-Client: Manifold
    http:
      timeoutSeconds: 30
      # proxyURL: http://proxy.local:3128
      tls:
        insecureSkipVerify: false
        # caFile: /etc/ssl/certs/ca-bundle.crt