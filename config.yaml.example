# Manifold Example Configuration

# Manifold Host
host: 'localhost'
port: 8080

# Manifold storage path: models, database files, etc
data_path: '/Users/yourusername/.manifold' # REPLACE with your actual path

# Set to true to automatically run llama-server instances for embeddings, reranker, and completions
# This enables the http://localhost:32186/v1/chat/completions endpoint running the gemma-3-4b-it model
single_node_instance: true

# Database Configuration (PGVector)
database:
  connection_string: "postgres://pgadmin:yourpassword@localhost:5432/manifold?sslmode=disable"  # REPLACE with your actual credentials

# HuggingFace Token
hf_token: "..."

# Google Gemini API token
google_gemini_key: "..."

# Anthropic API token
anthropic_key: "..."

# The completions, embeddings, and reranker services are automatically bootstrapped by Manifold in the ports defined below.
# 
# - Completions Service: Handles the generation of text completions based on input prompts.
# - Embeddings Service: Manages the creation of vector representations for text data.
# - Reranker Service: Reorders search results to improve relevance based on certain criteria.
#
# Each service is configured with default settings, but can be customized as needed.
# Users can also set remote hosts for these services if they are running on different machines.

# Example CLI command for running the completions service manually:
# llama-server -m <data_path>/models/gguf/gemma-3-4b-it.Q8_0.gguf --temp 1.0 --ctx-size 16384 --min-p 0.01 --top-p 0.95 --top-k 64 --repeat-penalty 1.0 -t -1 -ngl 99 --parallel 4 --batch-size 2048 --ubatch-size 512 --threads-http 4 -fa --host 127.0.0.1 --port 32186 --props
#
# Default Completions Configuration - using local gemma-3-4b-it model
completions:
  default_host: "http://127.0.0.1:32186/v1/chat/completions" # or https://api.openai.com/v1/chat/completions
  completions_model: 'gpt-4o' # ignored if using local endpoint
  api_key: "" # Used with OpenAI API if configured as default host
  # Agent configuration for ReAct framework
  agent:
    max_steps: 100  # Maximum number of steps the agent will take before terminating
    memory: false   # Legacy memory setting (will be deprecated)

# Example CLI command for running the embeddings service manually:
# llama-server -m <data_path>/models/embeddings/nomic-embed-text-v1.5.Q8_0.gguf -c 65536 -np 8 -b 8192 -ub 8192 -fa --host 127.0.0.1 --port 32184 -lv 1 --embedding
#
# Embeddings API Configuration
# Using local nomic-embed-text-v1.5
# The initialize process will automatically download and start the model at port 32184
embeddings:
  host: "http://127.0.0.1:32184/v1/embeddings"
  # OpenAI API compatible API key, not required for local servers unless configured on that server
  api_key: ""
  dimensions: 768 # Size of embedding dimensions
  embed_prefix: "search_document: "
  search_prefix: "search_query: "

# Example CLI command for running the reranker service manually:
# llama-server -m <data_path>/models/rerankers/slide-bge-reranker-v2-m3.Q4_K_M.gguf -c 65536 -np 8 -b 8192 -ub 8192 -fa --host 127.0.0.1 --port 32185 -lv 1 --reranking --pooling rank
#
# Reranker using local slide-bge-reranker-v2-m3
# The initialize process will automatically download and start the model at port 32185
reranker:
  host: "http://127.0.0.1:32185/v1/rerank"

# Agentic Memory Configuration
# Controls the long-term memory capabilities of the Manifold agent
agentic_memory:
  enabled: false  # Set to true to enable long-term memory across agent sessions
                  # When enabled, agents can recall past reasoning, actions, and observations
                  # This improves consistency and continuity in multi-turn interactions
                  # Requires PostgreSQL with pgvector extension for vector storage

# List of external MCP servers
# Example GitHub MCP server
mcpServers:
  github:
    command: docker
    args:
      - run
      - -i
      - --rm
      - -e
      - GITHUB_PERSONAL_ACCESS_TOKEN
      - ghcr.io/github/github-mcp-server
    env:
      GITHUB_PERSONAL_ACCESS_TOKEN: "<YOUR_TOKEN>"
  
  # Manifold's internal MCP server
  manifold:
    command: ./cmd/mcp-manifold/mcp-manifold
    args: []
    env: {}