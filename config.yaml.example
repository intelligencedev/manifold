# ===================================================================
# Manifold Configuration with Environment Variable Support
# ===================================================================
#
# This configuration file contains default values for Manifold.
# All settings can be overridden at runtime using environment variables.
#
# ENVIRONMENT VARIABLE MAPPING:
# ----------------------------
# Environment variables are automatically mapped to YAML paths using this convention:
#
# 1. Prefix all variables with "MANIFOLD__"
# 2. Use UPPERCASE for all letters 
# 3. Use DOUBLE underscore (__) to separate YAML hierarchy levels
# 4. Use SINGLE underscore (_) for keys containing underscores
#
# Examples:
#   MANIFOLD__HOST=api.example.com                    → host: 'api.example.com'
#   MANIFOLD__PORT=9000                               → port: 9000
#   MANIFOLD__DATABASE__CONNECTION_STRING=...         → database.connection_string: '...'
#   MANIFOLD__MCPSERVERS__GITHUB__COMMAND=docker      → mcpServers.github.command: 'docker'
#   MANIFOLD__COMPLETIONS__DEFAULT_HOST=http://...    → completions.default_host: 'http://...'
#
# VALUE TYPES:
# -----------
# Different value types are automatically handled as below:
#
# - Numbers:     MANIFOLD__PORT=8080                   → port: 8080
# - Booleans:    MANIFOLD__SINGLE_NODE_INSTANCE=false  → single_node_instance: false
# - Strings:     MANIFOLD__HOST=localhost              → host: 'localhost'
# - Null:        MANIFOLD__HF_TOKEN=null               → hf_token: null
# - JSON arrays: MANIFOLD__MCPSERVERS__GITHUB__ARGS='["run","--rm"]' 
#                → mcpservers.github.args: ["run","--rm"]
# - JSON objects: MANIFOLD__SOME__CONFIG='{"key":"value"}' 
#                → some.config: {"key":"value"}
#
# HOW IT WORKS:
# ------------
# At container startup, the process_config.sh script:
# 1. Copies this file to config.yaml
# 2. Finds all MANIFOLD__* environment variables
# 3. Maps them to their corresponding YAML paths
# 4. Updates the config.yaml file accordingly
#
# ===================================================================

# ===================================================================
# SERVER CONFIGURATION
# ===================================================================

# Server address and port
host: 'localhost'
port: 8080

# Storage path for models, database files, and other persistent data
data_path: '/data'

# ===================================================================
# RUNTIME CONFIGURATION
# ===================================================================

# When enabled, Manifold automatically runs llama-server instances for:
# - embeddings (port 32184)
# - reranker (port 32185)
# - completions (port 32186)
single_node_instance: true

# ===================================================================
# INGESTION CONFIGURATION
# ===================================================================

# Document ingestion settings
ingestion:
  # Maximum number of concurrent workers for processing chunks during ingestion
  # Controls parallelism for document splitting, summarization, keyword extraction, and embeddings
  # Environment variable: MANIFOLD__INGESTION__MAX_WORKERS
  max_workers: 4
  
  # Use advanced structure-aware splitting for better code organization
  # When enabled, preserves logical boundaries like function/class definitions, import blocks, etc.
  # Recommended for code repositories and structured documents
  # Environment variable: MANIFOLD__INGESTION__USE_ADVANCED_SPLITTING
  use_advanced_splitting: true

# ===================================================================
# DATABASE CONFIGURATION
# ===================================================================

database:
  # PostgreSQL connection string with PGVector extension
  # Format: postgres://username:password@hostname:port/database?sslmode=disable
  connection_string: ""

# ===================================================================
# Authentication
# ===================================================================

auth:
  # Generate a secure random key with: openssl rand -hex 32
  # Or use: head -c 32 /dev/urandom | base64
  secret_key: ""
  # Token expiry in hours
  token_expiry: 72

# ===================================================================
# API TOKENS
# ===================================================================

# HuggingFace API token for accessing gated models
hf_token: ""

# Google Gemini API token
google_gemini_key: ""

# Anthropic API token (Claude models)
anthropic_key: ""

# ===================================================================
# LLM SERVICES CONFIGURATION
# ===================================================================

# The completions endpoint for the proxy routes as well as agent mode.
# When agent mode is enabled, the endpoint configured in the frontend
# will be overridden by this.
# Due to CORS issues with some public endpoints, setting the completions
# endpoint in the frontend to the manifold host:port/v1/chat/completions
# will proxy the request to the endpoint configured below.
completions:
  # Any OpenAI compatible completions endpoint, local or remote
  default_host: "http://127.0.0.1:32186/v1/chat/completions"
  completions_model: 'gpt-4.1-mini' # ignored if using local endpoint
  api_key: "" # Used with OpenAI API if configured as default host
  agent:
    max_steps: 100
    memory: false
    num_tools: 5

# Example CLI command for running the embeddings service manually:
# llama-server -m <data_path>/models/embeddings/nomic-embed-text-v1.5.Q8_0.gguf -c 65536 -np 8 -b 8192 -ub 8192 -fa --host 127.0.0.1 --port 32184 -lv 1 --embedding
#
embeddings:
  # OpenAI-compatible API endpoint
  host: "http://127.0.0.1:32184/v1/embeddings"
  # API key for the embeddings service (if required)
  api_key: ""
  # Vector dimensions for the embedding model
  dimensions: 768
  # Prefix added to document text before embedding
  embed_prefix: "search_document: "
  # Prefix added to query text before embedding
  search_prefix: "search_query: "

# Reranker Service Configuration
# Reorders search results to improve relevance
reranker:
  # OpenAI-compatible API endpoint
  host: "http://127.0.0.1:32185/v1/rerank"

# Agentic Memory Configuration
# Controls the long-term memory capabilities of the Manifold agent
agentic_memory:
  enabled: false  # Set to true to enable long-term memory across agent sessions
                  # When enabled, agents can recall past reasoning, actions, and observations
                  # This improves consistency and continuity in multi-turn interactions
                  # Requires PostgreSQL with pgvector extension for vector storage

# Agent Fleet Configuration
# Define multiple cooperating agents
agent_fleet:
  workers:
    - name: assistant
      role: assistant
      model: gpt-4o
      endpoint: "http://127.0.0.1:32186/v1/chat/completions"
      ctx_size: 4096
      temperature: 0.7
      api_key: ""
      instructions: "You are a helpful assistant."
      max_steps: 20
      memory: false

# List of external MCP servers
  manifold:
    command: docker
    args:
      - run
      - --rm
      - -i
      - -e
      - DATA_PATH=/app/projects
      - -e
      - GIT_SSH_COMMAND=ssh -o StrictHostKeyChecking=no
      - -e
      - SSH_AUTH_SOCK=/ssh-agent
      - --volume
      - /Users/$USER/.manifold/tmp:/app/projects
      - --volume
      - $SSH_AUTH_SOCK:/ssh-agent # Put the value of the auth sock by running `echo $SSH_AUTH_SOCK`
      - -w
      - /app/projects
      - intelligencedev/manifold-mcp
    agent_name: manifold_tools
    instructions: |
      You are the tool agent for the "manifold" MCP server.
      Use only the tools from this server to answer requests.

  # Example GitHub MCP server
  # github:
  #   command: docker
  #   args:
  #     - run
  #     - -i
  #     - --rm
  #     - -e
  #     - GITHUB_PERSONAL_ACCESS_TOKEN
  #     - ghcr.io/github/github-mcp-server
  #   env:
  #     GITHUB_PERSONAL_ACCESS_TOKEN: ""