# Example env vars for manifold configuration. Use these to override
# fields in `config.yaml` when supported. Values left blank should be filled
# in for your environment. Lists can be represented as comma-separated values
# for simple cases.

# OpenAI / LLM provider
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=
OPENAI_SUMMARY_MODEL=
OPENAI_SUMMARY_URL=
# Optional: request-wide extra headers encoded as JSON or simple key:val pairs
OPENAI_EXTRA_HEADERS=
# Optional: extra params for request-level settings (JSON encoded)
OPENAI_EXTRA_PARAMS=

# Agent / runtime
WORKDIR=./sandbox
# Multiple workdirs can be comma-separated; config loader may expand these
WORKDIRS=./sandbox,/Users/art/projects/other
ENABLE_TOOLS=true
# Allow list for top-level tools (comma-separated). Empty = expose all tools
ALLOW_TOOLS=run_cli,web_search,write_file
# Maximum number of reasoning steps the agent can take (default: 8)
MAX_STEPS=8

# Runtime timeouts (seconds). 0 disables that specific server-side deadline.
# AGENT_RUN_TIMEOUT_SECONDS applies to non-streaming agent operations (/agent/run final, /api/prompt non-stream, vision).
# STREAM_RUN_TIMEOUT_SECONDS applies to streaming SSE paths (/agent/run, /api/prompt with Accept: text/event-stream or ?stream=1).
# WORKFLOW_TIMEOUT_SECONDS applies to /agent/workflow and /agent/specialist dispatches.
# Fallback chain:
#   streaming: STREAM_RUN_TIMEOUT_SECONDS -> AGENT_RUN_TIMEOUT_SECONDS -> 300 (default)
#   non-stream: AGENT_RUN_TIMEOUT_SECONDS -> 120 (default)
#   workflow: WORKFLOW_TIMEOUT_SECONDS -> AGENT_RUN_TIMEOUT_SECONDS -> 120 (default)
AGENT_RUN_TIMEOUT_SECONDS=0
STREAM_RUN_TIMEOUT_SECONDS=0
WORKFLOW_TIMEOUT_SECONDS=0

# Rolling summarization (opt-in)
SUMMARY_ENABLED=false
SUMMARY_THRESHOLD=40
SUMMARY_KEEP_LAST=12

# Execution / safety
# Comma-separated list of blocked binaries (matches config.exec.blockBinaries)
BLOCK_BINARIES=rm,sudo,chown,chmod,dd,mkfs,mount,umount
MAX_COMMAND_SECONDS=30
OUTPUT_TRUNCATE_BYTES=65536

# Observability / telemetry
OTEL_SERVICE_NAME=manifold
SERVICE_VERSION=
ENVIRONMENT=dev
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# Logging
LOG_PATH=
LOG_LEVEL=info
LOG_PAYLOADS=false

# Web integrations
WEB_SEARXNGURL=http://localhost:8080

# Databases (search/vector/graph). When DSN is empty the loader may default
# to in-memory backends. Example DSN: postgres://user:pass@host:5432/db?sslmode=disable
DATABASE_URL=
SEARCH_BACKEND=postgres
SEARCH_DSN=
SEARCH_INDEX=
VECTOR_BACKEND=postgres
VECTOR_DSN=
VECTOR_INDEX=
VECTOR_DIMENSIONS=1536
VECTOR_METRIC=cosine
GRAPH_BACKEND=postgres
GRAPH_DSN=

# Specialists: basic pattern for defining one via env. Many fields are easier
# to define inside config.yaml, but you can supply API keys and overrides here.
SPECIALISTS_0_NAME=image_analysis_expert
SPECIALISTS_0_BASEURL=https://api.openai.com/v1
SPECIALISTS_0_APIKEY=${OPENAI_API_KEY}
SPECIALISTS_0_MODEL=gpt-5-mini
SPECIALISTS_0_ENABLETOOLS=true

# MCP servers: examples for running external tool providers. Represented as
# a simple comma-separated list of server names; prefer declaring servers in
# `config.yaml` for complex options (command, args, env, keepAliveSeconds).
MCP_SERVERS=filesystem

# Misc: system prompt can be provided via file or env. If long, prefer a file.
SYSTEM_PROMPT_FILE=./system_prompt.txt
SYSTEM_PROMPT_SHORT=You are a state of the art intelligence.

# NOTE: Complex nested structures (specialists, routes, mcp.servers) are best
# defined in `config.yaml` or via dedicated environment parsing helpers.

# Web UI (embedded web server)
# Host and port the web UI binds to (overridable via WEB_UI_HOST / WEB_UI_PORT env vars)
WEB_UI_HOST=0.0.0.0
WEB_UI_PORT=8081
# Backend endpoint to forward prompt submissions to. Should accept POST with JSON {"prompt":"..."}
WEB_UI_BACKEND_URL=http:/localhost:32180/agent/run
