<!doctype html>
<html lang="en" data-page="chat">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Singularity UI</title>
  <link rel="stylesheet" href="/static/templates/styles.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
  <style>
    /* Avatar camera viewport */
    #avatarPane { display:flex; justify-content:center; }
    #avatarPane .avatar-window { width:256px; height:256px; overflow:hidden; position:relative; background:#0a0a0d; box-shadow:0 0 10px rgba(0,0,0,0.8) inset, 0 0 12px rgba(0,0,0,0.7); border-radius:12px; }
    #avatarPane .avatar-window img.agent-avatar { position:absolute; top:0; left:0; user-select:none; pointer-events:none; image-rendering:pixelated; filter:contrast(1.18) saturate(1.25) brightness(1.05) drop-shadow(0 0 6px rgba(0,255,220,0.08)); }
    /* CRT layers */
    #avatarPane .avatar-window.crt .crt-layer { position:absolute; inset:0; pointer-events:none; }
    /* Phosphor RGB triads (very subtle) */
    #avatarPane .avatar-window.crt .crt-phosphor { z-index:2; opacity:0.22; background:
      repeating-linear-gradient(to right,
        rgba(255,0,0,0.55) 0px, rgba(255,0,0,0.55) 1px,
        rgba(0,255,0,0.55) 1px, rgba(0,255,0,0.55) 2px,
        rgba(0,140,255,0.55) 2px, rgba(0,140,255,0.55) 3px,
        rgba(0,0,0,0.2) 3px, rgba(0,0,0,0.2) 4px
      ); mix-blend-mode:color-dodge; }
    /* Stronger scanlines */
    #avatarPane .avatar-window.crt .crt-scanlines { z-index:3; background:
      repeating-linear-gradient(to bottom,
        rgba(255,255,255,0.18) 0px,
        rgba(255,255,255,0.18) 1px,
        rgba(0,0,0,0.55) 1px,
        rgba(0,0,0,0.55) 2px
      ); mix-blend-mode:overlay; animation:crt-scan-move 5s linear infinite; }
    /* Darker vignette & curvature */
    #avatarPane .avatar-window.crt .crt-mask { z-index:4; background:
      radial-gradient(ellipse at center, rgba(0,0,0,0) 50%, rgba(0,0,0,0.35) 80%, rgba(0,0,0,0.75) 100%),
      linear-gradient(to right, rgba(0,0,0,0.45), rgba(0,0,0,0) 25%, rgba(0,0,0,0) 75%, rgba(0,0,0,0.45));
      mix-blend-mode:multiply; }
    /* Stronger but still subtle flicker */
    #avatarPane .avatar-window.crt .crt-flicker { z-index:5; background:rgba(255,255,255,0.04); animation:crt-flicker 0.08s infinite steps(2,end); mix-blend-mode:screen; }
    #avatarPane .avatar-window.crt .agent-avatar { z-index:1; }
    #avatarPane .avatar-window.crt { position:relative; }
    @keyframes crt-scan-move { 0% { background-position-y:0; } 100% { background-position-y:256px; } }
    @keyframes crt-flicker { 0%, 100% { opacity:0.02; } 50% { opacity:0.08; } }
  </style>
</head>
<body data-theme="dark">
  <header class="site-header">
    <div class="logo">intelligence.dev</div>
  </header>
  <main class="layout">
    <div class="col col-left">
      <!-- Metrics & navigation panel -->
      <aside id="metrics" class="panel">
        <div class="panel-header">Token Usage</div>
        <div id="metricsPane">
          <canvas id="tokenChart" width="320" height="180" aria-label="Token usage chart" role="img"></canvas>
          <div id="tokenLegend" class="legend"></div>
          <div class="metrics-footer" id="metricsFooter">--</div>
        </div>
      </aside>
    </div>
    <div class="col col-middle">
      <section id="chat" class="panel">
        <div class="panel-header">Conversation</div>
        <div id="chatPane" class="messages" aria-live="polite" aria-label="Chat transcript"></div>
      </section>
      <div class="composer-wrapper">
        <div class="composer-shell" id="composer">
          <!-- Multiline textarea (styled to match original input) enabling Shift+Enter newlines -->
          <textarea id="promptInput" rows="1" aria-label="Prompt" placeholder="Ask anything…  (Hold Space to talk)" autocomplete="off"></textarea>
          <button id="micBtn" type="button" aria-label="Record audio (Hold Space)" title="Voice: Hold Space to talk"><span class="rec-icon" aria-hidden="true"></span></button>
          <button id="submitBtn" type="button" data-state="idle" aria-pressed="false">▶ Run</button>
        </div>
      </div>
    </div>
    <div class="col col-right">
      <aside id="avatar" class="panel">
        <div class="panel-header">Project Manager</div>
        <div id="avatarPane">
          <div class="avatar-window crt" data-window-width="256" data-window-height="400">
            <!-- Default looping avatar (listening state) -->
            <img src="/assets/listening.gif" alt="Current Working Agent" class="agent-avatar" data-shake-amplitude="1" data-shake-speed="1" />
            <div class="crt-layer crt-phosphor"></div>
            <div class="crt-layer crt-scanlines"></div>
            <div class="crt-layer crt-mask"></div>
            <div class="crt-layer crt-flicker"></div>
          </div>
        </div>
      </aside>
      <aside id="tools" class="panel">
        <div class="panel-header">Tool Activity</div>
        <div id="toolsPane" class="tool-events" aria-live="polite" aria-label="Tool events">No tool activity yet.</div>
      </aside>
    </div>
  </main>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  <script>
// -------------------------------------------------------------
// Token Metrics Panel (polls /api/metrics/tokens every 2s)
// -------------------------------------------------------------
;(()=>{
  const endpoint = '/api/metrics/tokens'
  const canvas = document.getElementById('tokenChart')
  if(!canvas) return
  const ctx = canvas.getContext('2d')
  const legend = document.getElementById('tokenLegend')
  const footer = document.getElementById('metricsFooter')
  let history = [] // [{ts, models:{model:{p,c}}}]
  const MAX_POINTS = 60 // ~2 minutes window
  const COLORS = [ '#00c853', '#35e380', '#00b894', '#26de81', '#0fb9b1', '#2ecc71', '#1abc9c', '#2ed573' ]
  function fetchMetrics(){
    fetch(endpoint, {cache:'no-store'}).then(r=>r.json()).then(j=>{
      const ts = (j.timestamp||Date.now()/1000)*1000
      const map = {}
      ;(j.models||[]).forEach(m=>{ map[m.model] = {p:m.prompt, c:m.completion, t:m.total} })
      history.push({ts, map})
      if(history.length>MAX_POINTS) history.shift()
      draw()
    }).catch(()=>{})
  }
  function computeSeries(kind){ // kind: 'p' or 'c'
    // Collect set of models
    const models = new Set()
    history.forEach(h=>{ Object.keys(h.map).forEach(m=>models.add(m)) })
    const arr = [...models]
    return arr.map((model,i)=>{
      return {model, color: COLORS[i % COLORS.length], points: history.map(h=>({x:h.ts, y: h.map[model]? h.map[model][kind]: null}))}
    })
  }
  function draw(){
    const w = canvas.width, h = canvas.height
    ctx.clearRect(0,0,w,h)
    ctx.fillStyle = '#0d1a1f'
    ctx.fillRect(0,0,w,h)
    ctx.font = '10px JetBrains Mono, monospace'
    ctx.fillStyle = 'rgba(255,255,255,0.4)'
    ctx.textBaseline='top'
    if(history.length<2){ ctx.fillText('Collecting token metrics...', 8,8); return }
    // Determine max token value across prompt+completion deltas (per interval) for scale
    // We'll plot cumulative totals stacked as two lines per model (prompt & completion)
    // Simpler: plot prompt cumulative and completion cumulative separately (two passes)
    const seriesPrompt = computeSeries('p')
    const seriesCompletion = computeSeries('c')
    let maxY = 10
    seriesPrompt.concat(seriesCompletion).forEach(s=>{ s.points.forEach(p=>{ if(p.y!=null && p.y>maxY) maxY=p.y }) })
    // Grid
    ctx.strokeStyle='rgba(255,255,255,0.08)'; ctx.lineWidth=1
    for(let gx=0; gx<=5; gx++){ let x = gx/5*w; ctx.beginPath(); ctx.moveTo(x,0); ctx.lineTo(x,h); ctx.stroke() }
    for(let gy=0; gy<=4; gy++){ let y = gy/4*h; ctx.beginPath(); ctx.moveTo(0,y); ctx.lineTo(w,y); ctx.stroke(); let val = ((1-gy/4)*maxY)|0; ctx.fillStyle='rgba(255,255,255,0.25)'; ctx.fillText(String(val),2,y+2) }
    function plot(series, dash){
      series.forEach(s=>{
        ctx.save()
        if(dash){ ctx.setLineDash([4,3]) } else { ctx.setLineDash([]) }
        ctx.strokeStyle = s.color
        ctx.lineWidth = 1.5
        ctx.beginPath()
        s.points.forEach((p,idx)=>{ if(p.y==null) return; const x = idx/(history.length-1)*w; const y = h - (p.y/maxY)*h; if(idx===0) ctx.moveTo(x,y); else ctx.lineTo(x,y) })
        ctx.stroke()
        ctx.restore()
      })
    }
    plot(seriesPrompt,false)
    plot(seriesCompletion,true)
    // Legend
    legend.innerHTML = ''
    const models = new Map()
    seriesPrompt.forEach(s=>models.set(s.model,s.color))
    let idx=0
    models.forEach((color,model)=>{
      const wrap = document.createElement('div')
      wrap.className='legend-item'
      wrap.innerHTML = `<span class="swatch" style="--c:${color}"></span><span class="name">${model}</span>`
      legend.appendChild(wrap); idx++
    })
    footer.textContent = 'Prompt: solid line • Completion: dashed line'
  }
  fetchMetrics()
  setInterval(fetchMetrics, 2000)
})()

// Auto-scroll management for chat pane
let autoScrollEnabled = true;
const chatPane = document.getElementById('chatPane');
chatPane.addEventListener('scroll', () => {
  const isAtBottom = chatPane.scrollTop + chatPane.clientHeight >= chatPane.scrollHeight - 10; // 10px tolerance
  autoScrollEnabled = isAtBottom;
});
// Camera shake with fixed viewport window + crossfade API (persistent across swaps)
(()=>{
  const windowEl = document.querySelector('#avatarPane .avatar-window')
  let currentImg = windowEl?.querySelector('img.agent-avatar')
  if (!windowEl || !currentImg) return
  const amplitude = parseFloat(currentImg.dataset.shakeAmplitude || '16')
  const speed = parseFloat(currentImg.dataset.shakeSpeed || '1')
  const freqX = [0.9, 2.2, 4.7]
  const freqY = [1.1, 2.0, 3.8]
  const weights = [0.6, 0.3, 0.1]
  const phasesX = freqX.map(()=>Math.random()*Math.PI*2)
  const phasesY = freqY.map(()=>Math.random()*Math.PI*2)
  function sample(t){
    t *= speed
    let nx=0, ny=0
    for(let i=0;i<3;i++){ nx += weights[i]*Math.sin(2*Math.PI*freqX[i]*t + phasesX[i]); ny += weights[i]*Math.sin(2*Math.PI*freqY[i]*t + phasesY[i]); }
    return [nx*amplitude, ny*amplitude]
  }
  function sizeImage(image){
    if(!image) return
    const ww = parseInt(windowEl.dataset.windowWidth || windowEl.clientWidth || 256, 10)
    const wh = parseInt(windowEl.dataset.windowHeight || windowEl.clientHeight || 256, 10)
    windowEl.style.width = ww + 'px'
    windowEl.style.height = wh + 'px'
    const nw = image.naturalWidth || ww
    const nh = image.naturalHeight || wh
    const scale = Math.max((ww+2*amplitude)/nw, (wh+2*amplitude)/nh)
    const targetW = nw * scale
    const targetH = nh * scale
    image.style.width = targetW + 'px'
    image.style.height = targetH + 'px'
    const baseX = (ww - targetW)/2
    const baseY = (wh - targetH)/2
    image.dataset._baseX = baseX
    image.dataset._baseY = baseY
  }
  if (currentImg.complete) sizeImage(currentImg); else currentImg.addEventListener('load', ()=>sizeImage(currentImg))
  window.addEventListener('resize', ()=>sizeImage(currentImg))
  let running = true
  const start = performance.now()
  function loop(now){
    if(!running) return
    const t = (now-start)/1000
    const [ox, oy] = sample(t)
    if (currentImg) {
      const baseX = parseFloat(currentImg.dataset._baseX||'0')
      const baseY = parseFloat(currentImg.dataset._baseY||'0')
      currentImg.style.transform = `translate(${(baseX+ox).toFixed(2)}px, ${(baseY+oy).toFixed(2)}px)`
    }
    requestAnimationFrame(loop)
  }
  requestAnimationFrame(loop)
  document.addEventListener('visibilitychange', ()=>{ running = !document.hidden; if (running) requestAnimationFrame(loop) })
  window.setAvatarImage = function(src, fadeMs=400){
    const prev = currentImg
    const next = new Image()
    next.src = src
    next.alt = prev?.alt || 'Agent'
    next.className = prev?.className || 'agent-avatar'
    next.dataset.shakeAmplitude = prev?.dataset.shakeAmplitude || String(amplitude)
    next.dataset.shakeSpeed = prev?.dataset.shakeSpeed || String(speed)
    next.style.position = 'absolute'
    next.style.top = '0'; next.style.left = '0'
    next.style.opacity = '0'
    next.style.transition = `opacity ${fadeMs}ms ease`
    const firstOverlay = windowEl.querySelector('.crt-layer')
    if (firstOverlay) windowEl.insertBefore(next, firstOverlay); else windowEl.appendChild(next)
    next.addEventListener('load', ()=>{
      sizeImage(next)
      // Switch the shaking target to the new image immediately so effect persists
      currentImg = next
      requestAnimationFrame(()=>{ next.style.opacity = '1' })
      setTimeout(()=>{ prev?.remove() }, fadeMs+40)
    })
  }
})()

// Sliding in-container code copy button: stays visible while scrolling large blocks, clamps inside block
function addCopyButtonsToCodeBlocks(container) {
  const chat = document.getElementById('chatPane')
  if (!window.__codeCopyScrollInit) {
    window.__codeCopyScrollInit = true
    let scheduled = false
    function schedule() {
      if (scheduled) return
      scheduled = true
      requestAnimationFrame(()=>{ scheduled = false; repositionAll() })
    }
    function repositionAll() {
      document.querySelectorAll('.msg.agent pre').forEach(pre => {
        const btn = pre.querySelector(':scope > .code-copy-btn')
        if (btn) positionCodeCopyButton(pre, btn)
      })
    }
    window.__repositionAllCodeCopy = repositionAll
    const scrollTargets = [chat, window]
    scrollTargets.forEach(t=> t && t.addEventListener('scroll', schedule, {passive:true}))
    window.addEventListener('resize', schedule)
  }

  function positionCodeCopyButton(pre, btn) {
    const chat = document.getElementById('chatPane')
    if (!chat) return
    const chatRect = chat.getBoundingClientRect()
    const preRect = pre.getBoundingClientRect()
    const btnH = btn.offsetHeight || 24
    // If pre not visible skip
    if (preRect.bottom < chatRect.top || preRect.top > chatRect.bottom) return
    const hiddenTop = Math.max(0, chatRect.top - preRect.top)
    let y = 8 + hiddenTop
    const maxY = preRect.height - btnH - 8
    if (y > maxY) y = maxY
    btn.style.top = y + 'px'
  }

  const pres = container.querySelectorAll('pre')
  pres.forEach(pre => {
    if (pre.querySelector(':scope > .code-copy-btn')) return
    const code = pre.querySelector('code')
    if (!code) return
  const copyBtn = document.createElement('button')
  copyBtn.className = 'copy-btn code-copy-btn'
  copyBtn.type = 'button'
  copyBtn.title = 'Copy code'
  copyBtn.setAttribute('aria-label','Copy code block')
  copyBtn.innerHTML = '<span class="label">Copy</span>'
    copyBtn.style.top = '8px'
    copyBtn.style.right = '8px'
    copyBtn.addEventListener('click', e => {
      e.stopPropagation()
      navigator.clipboard.writeText(code.textContent).then(()=>{
  copyBtn.innerHTML = '<span class="label">Copied</span>'
        copyBtn.classList.add('copied')
  setTimeout(()=>{ copyBtn.innerHTML = '<span class="label">Copy</span>'; copyBtn.classList.remove('copied') }, 1200)
      })
    })
    pre.appendChild(copyBtn)
    positionCodeCopyButton(pre, copyBtn)
  })
  if (window.__repositionAllCodeCopy) window.__repositionAllCodeCopy()
}

// Helper: append message to chat pane (renders markdown for agent)
function appendMessage(kind, text, raw=false) {
  const el = document.createElement('div')
  const content = document.createElement('div')
  const timestamp = document.createElement('div')
  timestamp.className = 'timestamp'
  timestamp.textContent = new Date().toLocaleTimeString()
  // Make sure we have the chat container (streaming previously failed when this was undefined)
  const chat = document.getElementById('chatPane')
  if (kind === 'user') {
    el.className = 'msg user'
    content.textContent = text
    el.appendChild(content)
    el.appendChild(timestamp)
  } else if (kind === 'agent') {
    el.className = 'msg agent'
    el.dataset.rawMarkdown = text || '' // Store raw markdown for streaming updates
    if (raw) {
      content.textContent = text
    } else {
      content.innerHTML = marked.parse(text || '')
      addCopyButtonsToCodeBlocks(content)
    }
    el.appendChild(content)
    // Add copy button for agent messages (appears on hover, ChatGPT-like)
    const copyBtn = document.createElement('button')
    copyBtn.className = 'copy-btn'
    copyBtn.type = 'button'
    copyBtn.setAttribute('aria-label', 'Copy message')
    copyBtn.innerHTML = '<span class="label">Copy</span>'
    copyBtn.addEventListener('click', () => {
      const toCopy = el.dataset.rawMarkdown || content.textContent || ''
      navigator.clipboard.writeText(toCopy).then(() => {
        copyBtn.classList.add('copied')
        copyBtn.innerHTML = '<span class="label">Copied</span>'
        setTimeout(() => {
          copyBtn.classList.remove('copied')
          copyBtn.innerHTML = '<span class="label">Copy</span>'
        }, 1600)
      }).catch(()=>{
        copyBtn.innerHTML = '<span class="icon">!</span><span class="label">Error</span>'
        setTimeout(() => {
          copyBtn.classList.remove('copied')
          copyBtn.innerHTML = '<span class="label">Copy</span>'
        }, 1600)
      })
    })
    el.appendChild(copyBtn)
    el.appendChild(timestamp)
  } else if (kind === 'info') {
    el.className = 'msg info'
    content.textContent = text
    el.appendChild(content)
    el.appendChild(timestamp)
  }
  if (chat) chat.appendChild(el)
  if (autoScrollEnabled) {
    chat && (chat.scrollTop = chat.scrollHeight)
  }
}

function appendToolEvent(title, content) {
  const pane = document.getElementById('toolsPane')
  // If first time, clear default text
  if (pane.textContent.trim() === 'No tool activity yet.') pane.textContent = ''
  const wrapper = document.createElement('div')
  wrapper.className = 'tool-block'
  const h = document.createElement('h3')
  h.textContent = title
  const pre = document.createElement('pre')
  pre.textContent = content
  wrapper.appendChild(h)
  wrapper.appendChild(pre)
  pane.appendChild(wrapper)
  pane.scrollTop = pane.scrollHeight
}

let currentInference = {
  controller: null,
  running: false,
  startedAt: 0
}

function setSubmitState(state){
  const btn = document.getElementById('submitBtn')
  if(!btn) return
  if(state === 'running'){
    btn.textContent = '■ Stop'
    btn.dataset.state = 'running'
    btn.classList.add('stop')
    btn.setAttribute('aria-pressed','true')
  } else {
    btn.textContent = '▶ Run'
    btn.dataset.state = 'idle'
    btn.classList.remove('stop')
    btn.setAttribute('aria-pressed','false')
  }
}

async function submitPrompt(prompt) {
  // If already running => this call is treated as cancel request
  if(currentInference.running){
    if(currentInference.controller){ currentInference.controller.abort() }
    appendMessage('info','(stopping inference)')
    return
  }
  currentInference.controller = new AbortController()
  currentInference.running = true
  currentInference.startedAt = performance.now()
  setSubmitState('running')
  // Trigger avatar animation immediately when a prompt is submitted
  if (typeof window.playPromptAnimation === 'function') {
    window.playPromptAnimation()
  }
  appendMessage('user', prompt)
  const backend = '/api/prompt'

  // Try SSE first by requesting Accept: text/event-stream
  let sseRes
  try {
    sseRes = await fetch(backend, {
      method: 'POST', headers: {'Content-Type':'application/json', 'Accept':'text/event-stream'},
      body: JSON.stringify({prompt}), signal: currentInference.controller.signal
    })
  } catch (err) {
    if(err.name === 'AbortError'){
      appendMessage('info','(inference cancelled)')
    } else {
      appendMessage('agent','Network error: '+ err.message, true)
    }
    currentInference.running = false
    setSubmitState('idle')
    return
  }

  const ct = sseRes.headers.get('content-type') || ''
  if (sseRes.status >= 400) {
    let txt
    try { txt = await sseRes.text() } catch { txt = '' }
    appendMessage('agent', `Error (${sseRes.status} ${sseRes.statusText}) calling /api/prompt.\n${txt || '(no body)'}\nHint: Make sure the web UI server (cmd/webui) is running and not conflicting with agentd on the same port. If you opened index.html directly from the file system, start the web UI server instead.`, true)
    return
  }

  if (ct.includes('text/event-stream')) {
    // Stream with streaming parser
  const reader = sseRes.body.getReader()
    const dec = new TextDecoder()
    let buf = ''
    appendMessage('agent', '') // placeholder for streaming content
    while (true) {
      let readResult
      try {
        readResult = await reader.read()
      } catch (e) {
        if(e.name === 'AbortError') {
          appendMessage('info','(stream cancelled)')
        }
        break
      }
      const { value, done } = readResult
      if (done) break
      buf += dec.decode(value, {stream: true})
      // SSE parsing: events separated by double newline
      let idx
      while ((idx = buf.indexOf('\n\n')) !== -1) {
        // Extract one SSE event block without aggressive trimming to preserve leading spaces in deltas
        let raw = buf.slice(0, idx)
        buf = buf.slice(idx+2)
        // Remove trailing single CR if present (Windows line endings) but keep leading spaces
        if (raw.endsWith('\r')) raw = raw.slice(0, -1)
        // Each line in raw starts with "data: " optionally
        const lines = raw.split(/\r?\n/)
        lines.forEach(line=>{
          if (line.startsWith('data:')) {
            // Per SSE spec, a single optional space may follow 'data:'; preserve any further leading spaces from model output
            let payload = line.slice(5)
            if (payload.startsWith(' ')) payload = payload.slice(1)
            // Expect JSON payloads with {type: 'delta'|'tool'|'final', data: '...'}
            try {
              const obj = JSON.parse(payload)
              if (obj.type === 'delta') {
                // Append to last agent message
                const chat = document.getElementById('chatPane')
                const last = chat.lastElementChild
                if (last && last.dataset.rawMarkdown !== undefined) {
                  // Append to raw markdown and re-render
                  if (last.dataset.rawMarkdown === '') {
                    // First delta: keep model's leading spaces except pure leading newlines
                    last.dataset.rawMarkdown = (obj.data||'').replace(/^[\r\n]+/, '')
                  } else {
                    let incoming = obj.data || ''
                    // Heuristic: if previous char ends a sentence and incoming starts a capital/quote without leading space, insert one
                    if (incoming && last.dataset.rawMarkdown) {
                      const prevChar = last.dataset.rawMarkdown.slice(-1)
                      // Typical sentence terminators
                      if(/[.!?]/.test(prevChar)) {
                        // If incoming starts with letter/quote/parenthesis and not a space or newline
                        if(!/^[ \t\n]/.test(incoming) && /^[A-Z"'(\[]/.test(incoming)) {
                          incoming = ' ' + incoming
                        }
                      }
                    }
                    last.dataset.rawMarkdown += incoming
                  }
                  const contentDiv = last.querySelector('div');
                  if (contentDiv) {
                    contentDiv.innerHTML = marked.parse(last.dataset.rawMarkdown);
                    addCopyButtonsToCodeBlocks(contentDiv);
                  }
                  if (autoScrollEnabled) {
                    chat.scrollTop = chat.scrollHeight
                  }
                }
              } else if (obj.type === 'tts_chunk') {
                // Real-time TTS PCM frame chunk (raw 16-bit little-endian PCM @16k mono)
                if (!window.__tts) {
                  window.__tts = { pcm: [], bytes:0, rebuilding:false, audio:new Audio(), lastBuild:0 }
                  window.__tts.audio.autoplay = true
                }
                if (obj.b64) {
                  const frames = Uint8Array.from(atob(obj.b64), c=>c.charCodeAt(0))
                  window.__tts.pcm.push(frames)
                  window.__tts.bytes += frames.length
                  // Debounce rebuild (~120ms) to limit object URLs
                  const now = performance.now()
                  if (now - window.__tts.lastBuild > 110) {
                    window.__tts.lastBuild = now
                    rebuildStreamingAudio()
                  } else {
                    clearTimeout(window.__tts._t)
                    window.__tts._t = setTimeout(()=>{ window.__tts.lastBuild = performance.now(); rebuildStreamingAudio() }, 120)
                  }
                }
              } else if (obj.type === 'tts_audio') {
                // Final file available; prefer continuous stream if already playing
                try {
                  if (!window.__tts || window.__tts.bytes < 8000) { // fallback if few frames
                    const a = new Audio(obj.url)
                    a.play().catch(()=>{})
                  }
                } catch(_e){}
              } else if (obj.type === 'tool_start') {
                // Create a placeholder block for this tool (store element id)
                const pane = document.getElementById('toolsPane')
                if (pane.textContent.trim() === 'No tool activity yet.') pane.textContent = ''
                const wrapper = document.createElement('div')
                wrapper.className = 'tool-block'
                // Use tool_id if present for later lookup
                const tid = obj.tool_id || ('tid_' + Math.random().toString(36).slice(2))
                wrapper.dataset.toolId = tid
                const h = document.createElement('h3')
                h.textContent = obj.title || 'Tool'
                const pre = document.createElement('pre')
                pre.textContent = '(running...)' + (obj.args ? '\nargs: ' + obj.args : '')
                wrapper.appendChild(h)
                wrapper.appendChild(pre)
                pane.appendChild(wrapper)
                pane.scrollTop = pane.scrollHeight
              } else if (obj.type === 'tool_result') {
                // Find existing block by tool_id; if not found create new
                const pane = document.getElementById('toolsPane')
                let wrapper = null
                if (obj.tool_id) {
                  wrapper = pane.querySelector('[data-toolid="' + obj.tool_id + '"]')
                }
                if (!wrapper) {
                  appendToolEvent(obj.title || 'Tool', obj.data || '')
                } else {
                  const pre = wrapper.querySelector('pre') || document.createElement('pre')
                  pre.textContent = obj.data || ''
                  if (!pre.parentElement) wrapper.appendChild(pre)
                  pane.scrollTop = pane.scrollHeight
                }
              } else if (obj.type === 'tool') {
                // Backward compatibility single combined event
                appendToolEvent(obj.title || 'Tool', obj.data || '')
              } else if (obj.type === 'final') {
                // replace last element with final rendered markdown
                const chat = document.getElementById('chatPane')
                const last = chat.lastElementChild
                if (last && last.dataset.rawMarkdown !== undefined) {
                  last.dataset.rawMarkdown = obj.data || ''
                  const contentDiv = last.querySelector('div');
                  if (contentDiv) {
                    contentDiv.innerHTML = marked.parse(last.dataset.rawMarkdown);
                    addCopyButtonsToCodeBlocks(contentDiv);
                  }
                }
              }
            } catch (e) {
              // Not JSON, append raw
              appendMessage('agent', payload, false)
            }
          }
        })
      }
    }
    // process any remaining buffer
    if (buf.trim() !== '') {
      try {
        const obj = JSON.parse(buf.trim()) // final leftover (safe to trim here; it's a standalone JSON object)
        if (obj.type === 'final') appendMessage('agent', obj.data || '', false)
      } catch (e) {
        appendMessage('agent', buf, true)
      }
    }
  currentInference.running = false
  setSubmitState('idle')
  return
  }

  // Fallback: non-streaming
  const ctype = sseRes.headers.get('content-type') || ''
  if (ctype.includes('application/json')) {
    const j = await sseRes.json()
    if (j.result) appendMessage('agent', j.result, false)
    else appendMessage('agent', JSON.stringify(j), true)
  } else {
    const txt = await sseRes.text()
    appendMessage('agent', txt, true)
  }
  currentInference.running = false
  setSubmitState('idle')
}

// Build/refresh a playable WAV from accumulated raw PCM frames
function rebuildStreamingAudio(){
  const t = window.__tts
  if (!t || t.rebuilding) return
  t.rebuilding = true
  try {
    // Flatten frames
    let total = 0; for (const c of t.pcm) total += c.length
    const merged = new Uint8Array(total)
    let off=0; for (const c of t.pcm){ merged.set(c, off); off+=c.length }
    // Construct minimal 44-byte WAV header (PCM 16-bit mono 16k)
    const sampleRate = 16000, channels=1, bits=16
    const dataSize = merged.length
    const buffer = new ArrayBuffer(44 + dataSize)
    const view = new DataView(buffer)
    let p=0
    function w8(v){ view.setUint8(p++, v) }
    function w16(v){ view.setUint16(p, v, true); p+=2 }
    function w32(v){ view.setUint32(p, v, true); p+=4 }
    ;['R','I','F','F'].forEach(c=>w8(c.charCodeAt(0)))
    w32(36 + dataSize)
    ;['W','A','V','E'].forEach(c=>w8(c.charCodeAt(0)))
    ;['f','m','t',' '].forEach(c=>w8(c.charCodeAt(0)))
    w32(16)      // fmt chunk size
    w16(1)       // PCM
    w16(channels)
    w32(sampleRate)
    const byteRate = sampleRate * channels * bits/8
    w32(byteRate)
    w16(channels * bits/8) // block align
    w16(bits)
    ;['d','a','t','a'].forEach(c=>w8(c.charCodeAt(0)))
    w32(dataSize)
    new Uint8Array(buffer, 44).set(merged)
    const blob = new Blob([buffer], {type:'audio/wav'})
    const url = URL.createObjectURL(blob)
    if (t.audio.src) URL.revokeObjectURL(t.audio.src)
    t.audio.src = url
    t.audio.play().catch(()=>{})
  } finally {
    t.rebuilding = false
  }
}

document.getElementById('submitBtn').addEventListener('click', async () => {
  const btn = document.getElementById('submitBtn')
  if(btn.dataset.state === 'running'){
    // Acts as cancel
    if(currentInference.controller) currentInference.controller.abort()
    return
  }
  const input = document.getElementById('promptInput')
  const v = input.value.trim()
  if (!v) return
  input.value = ''
  // Reset height back to original single-line after submission
  input.style.height = 'auto'
  // Remove explicit height so CSS + rows attribute governs
  input.style.removeProperty('height')
  input.rows = 1
  input.scrollTop = 0
  try { await submitPrompt(v) } catch (err) { appendMessage('agent','Error: '+err.message,true); currentInference.running=false; setSubmitState('idle') }
})

// -------------------------------------------------------------
// Speech to Text (Web MediaRecorder -> WAV -> /stt)
// -------------------------------------------------------------
;(()=>{
  const micBtn = document.getElementById('micBtn')
  if(!micBtn || !navigator.mediaDevices) return
  let mediaRecorder = null
  let chunks = []
  let recording = false
  let startTime = 0
  function setState(rec){
  recording = rec
  // Toggle recording class; icon styling handled purely by CSS
  if(rec) micBtn.classList.add('recording'); else micBtn.classList.remove('recording')
  }
  async function start(){
    try {
      const stream = await navigator.mediaDevices.getUserMedia({audio:true})
      chunks = []
      mediaRecorder = new MediaRecorder(stream, {mimeType: 'audio/webm'})
      mediaRecorder.ondataavailable = e=>{ if(e.data && e.data.size>0) chunks.push(e.data) }
      mediaRecorder.onstop = ()=>{ stream.getTracks().forEach(t=>t.stop()); process() }
      mediaRecorder.start()
      startTime = performance.now()
      setState(true)
    } catch(err){
      appendMessage('agent', 'Microphone error: '+ err.message, true)
    }
  }
  function stop(){
    if(mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop()
    setState(false)
  }
  async function process(){
    if(!chunks.length){ appendMessage('agent','No audio captured.',true); return }
    // Convert webm/opus to PCM WAV via browser decode (AudioContext)
    try {
      const blob = new Blob(chunks, {type: 'audio/webm'})
      const arrayBuf = await blob.arrayBuffer()
      const audioCtx = new (window.AudioContext||window.webkitAudioContext)({sampleRate:16000})
      const decoded = await audioCtx.decodeAudioData(arrayBuf)
      // Resample (if needed) to 16k mono
      const targetRate = 16000
      const chData = decoded.numberOfChannels>1 ? (()=>{ // mixdown
        const left = decoded.getChannelData(0)
        const right = decoded.getChannelData(1)
        const mixed = new Float32Array(left.length)
        for(let i=0;i<mixed.length;i++){ mixed[i]=(left[i]+right[i])/2 }
        return mixed
      })() : decoded.getChannelData(0)
      const ratio = decoded.sampleRate / targetRate
      const outLen = Math.round(chData.length / ratio)
      const resampled = new Float32Array(outLen)
      for(let i=0;i<outLen;i++){ resampled[i] = chData[Math.min(chData.length-1, Math.round(i*ratio))] }
      // Encode minimal WAV (16-bit PCM)
      function floatTo16(f){
        const s = Math.max(-1, Math.min(1, f))
        return s<0 ? s*0x8000 : s*0x7FFF
      }
      const wavBuffer = new ArrayBuffer(44 + resampled.length*2)
      const view = new DataView(wavBuffer)
      let off=0
      function writeStr(s){ for(let i=0;i<s.length;i++) view.setUint8(off++, s.charCodeAt(i)) }
      function write16(v){ view.setUint16(off, v, true); off+=2 }
      function write32(v){ view.setUint32(off, v, true); off+=4 }
      writeStr('RIFF'); write32(36 + resampled.length*2); writeStr('WAVE')
      writeStr('fmt '); write32(16); write16(1); write16(1); write32(targetRate); write32(targetRate*2); write16(2); write16(16)
      writeStr('data'); write32(resampled.length*2)
      for(let i=0;i<resampled.length;i++){ view.setInt16(off, floatTo16(resampled[i]), true); off+=2 }
      const wavBlob = new Blob([wavBuffer], {type:'audio/wav'})
      const form = new FormData()
      form.append('audio', wavBlob, 'capture.wav')
      appendMessage('agent','Transcribing...', true)
      const resp = await fetch('/stt', {method:'POST', body: form})
      if(!resp.ok){ const t = await resp.text(); appendMessage('agent','STT error: '+resp.status+' '+t,true); return }
      const j = await resp.json()
      const text = (j.text||'').trim()
      if(text){
        const input = document.getElementById('promptInput')
        if(input.value) input.value += ' '
        input.value += text
        appendMessage('user','(speech)', text)
      } else {
        appendMessage('agent','No speech detected.', true)
      }
    } catch(err){
      appendMessage('agent','STT processing failed: '+err.message,true)
    }
  }
  micBtn.addEventListener('click', ()=>{ recording ? stop() : start() })
  // ---------------------------------------------------------
  // Voice hotkey:
  //   Space (hold) -> push-to-talk (start on keydown, stop on keyup)
  // ---------------------------------------------------------
  const urlParams = new URLSearchParams(location.search)
  const debugKeys = urlParams.has('debugKeys')
  let spacePTT = false

  function isEditable(el){
    if(!el) return false
    const tag = el.tagName
    if(tag === 'INPUT' || tag === 'TEXTAREA') return true
    if(el.isContentEditable) return true
    return false
  }

  window.addEventListener('keydown', (e)=>{
    if(debugKeys){ console.debug('[VoiceHotkey keydown]', e.key, {alt:e.altKey, ctrl:e.ctrlKey, meta:e.metaKey, shift:e.shiftKey, target:e.target}) }
    const activeEl = document.activeElement
  // Space (push-to-talk) when not focused on editable element
  if(e.code === 'Space' && !e.repeat && !isEditable(activeEl)) {
      e.preventDefault()
      if(!recording){ spacePTT = true; start() }
      return
    }
  }, {capture:true})

  window.addEventListener('keyup', (e)=>{
    if(debugKeys){ console.debug('[VoiceHotkey keyup]', e.key) }
  if(e.code === 'Space' && spacePTT){
      e.preventDefault(); spacePTT = false; if(recording) stop()
    }
  }, {capture:true})
})()

// Press Enter to submit
;(function(){
  const el = document.getElementById('promptInput')
  if(!el) return
  const MAX_HEIGHT_PX = 180 // cap growth (approx ~8-10 lines depending on line-height)
  function autoResize(){
    el.style.height = 'auto'
    const h = Math.min(el.scrollHeight, MAX_HEIGHT_PX)
    el.style.height = h + 'px'
  }
  el.addEventListener('input', autoResize)
  autoResize()
  el.addEventListener('keydown', (e)=>{
    if (e.key === 'Enter' && !e.shiftKey && !e.isComposing) {
      e.preventDefault()
      document.getElementById('submitBtn').click()
    }
    // Shift+Enter: allow default newline insertion
  })
})()

// Theme toggle removed (single theme mode)

// -------------------------------------------------------------
// Avatar Animation System
// Idle state: continuously cycle through listening*.gif variants, selecting a new one
// after the previous finishes a single loop (estimated via frame delays).
// Active (prompt) state: play a one-off reaction GIF (thinking/listening2/etc.) for one loop
// then return to idle cycle (not just a single default).\n
// -------------------------------------------------------------
;(()=>{
  // Idle pool (listening variants). If new listening*.gif files are added they can be appended here manually
  const IDLE_GIFS = [
    '/assets/listening.gif',
    '/assets/listening2.gif',
    '/assets/listening3.gif',
    '/assets/listening4.gif'
  ]
  // Prompt reaction pool (non-idle / expressive set)
  const PROMPT_GIFS = [
    '/assets/thinking.gif',
    '/assets/pensive.gif',
    '/assets/smiling.gif',
    '/assets/sad.gif'
  ].filter(Boolean)
  // Current operational mode
  let inPromptAnimation = false
  // Fallback duration per GIF if parsing fails (ms)
  const DEFAULT_DURATION = 2500
  // Cache of measured single-loop durations
  const gifDurations = {}
  let revertTimer = null
  let currentGif = null
  let idleTimer = null
  let lastIdleChoice = null

  async function measureGifOnce(url){
    if (gifDurations[url]) return gifDurations[url]
    try {
      const res = await fetch(url + '?cache=meta', {cache:'no-store'})
      const buf = await res.arrayBuffer()
      const bytes = new Uint8Array(buf)
      let total = 0
      for (let i=0; i < bytes.length - 9; i++) {
        // Graphic Control Extension: 21 F9 04 .. .. delay(lo) delay(hi)
        if (bytes[i]===0x21 && bytes[i+1]===0xF9 && bytes[i+2]===0x04) {
          const delayLo = bytes[i+4]
          const delayHi = bytes[i+5]
            // delay in hundredths of a second
          let delay = delayLo | (delayHi << 8)
          if (delay === 0) delay = 1 // some gifs put 0; treat as 10ms
          total += delay * 10 // ms
        }
      }
      if (total === 0) total = DEFAULT_DURATION
      gifDurations[url] = total
      return total
    } catch (e) {
      gifDurations[url] = DEFAULT_DURATION
      return DEFAULT_DURATION
    }
  }

  function pickRandom(list, exclude){
    if(!list.length) return null
    let filtered = list
    if (exclude) filtered = list.filter(x=>x!==exclude)
    if(!filtered.length) filtered = list
    return filtered[Math.floor(Math.random()*filtered.length)]
  }

  async function scheduleNextIdle(afterMs){
    if (idleTimer) clearTimeout(idleTimer)
    idleTimer = setTimeout(()=>{ runIdleCycle() }, afterMs)
  }

  async function runIdleCycle(){
    if (inPromptAnimation) return // paused during prompt reaction
    const choice = pickRandom(IDLE_GIFS, lastIdleChoice)
    if(!choice) return
    lastIdleChoice = choice
    currentGif = choice
    const duration = await measureGifOnce(choice)
    window.setAvatarImage(choice + '?idle=' + Date.now(), 220)
    scheduleNextIdle(Math.min(duration, 10000))
  }

  async function playPromptAnimation(){
    if(!window.setAvatarImage) return
    if (!PROMPT_GIFS.length) return
    inPromptAnimation = true
    if (revertTimer) { clearTimeout(revertTimer); revertTimer = null }
    if (idleTimer) { clearTimeout(idleTimer); idleTimer = null }
    const choice = pickRandom(PROMPT_GIFS)
    currentGif = choice
    const duration = await measureGifOnce(choice)
    window.setAvatarImage(choice + '?prompt=' + Date.now(), 160)
    revertTimer = setTimeout(()=>{
      if (currentGif === choice) {
        inPromptAnimation = false
        currentGif = null
        runIdleCycle() // resume idle cycle immediately
      }
      revertTimer = null
    }, Math.min(duration, 8000))
  }

  // Expose globally for submitPrompt hook
  window.playPromptAnimation = playPromptAnimation

  window.addEventListener('load', ()=>{
    // Start idle cycle (replace whatever initial avatar is after first loop)
    runIdleCycle()
  })
})()
</script>
</body>
</html>
